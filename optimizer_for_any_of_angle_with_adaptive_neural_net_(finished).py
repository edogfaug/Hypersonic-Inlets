# -*- coding: utf-8 -*-
"""Optimizer for any # of angle with adaptive Neural Net (finished)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SMkcGDKrKSEXUbOnRrSICF7CMLePlmaC

Please delete runtimes before every test to reset the neural net!

### **Given Functions**
"""

## Given Functions

## Given functions provided by W&M
# Needed as pre req for other coding functions used

import matplotlib.pyplot as plt
import csv
import math
import numpy as np

def xy(ethetas):
    thetas = np.cumsum(ethetas)
    xs = np.arange(0,len(ethetas))
    ys = np.repeat(-1.0,len(ethetas))
    ys[0] = 0
    tans = np.tan(thetas*np.pi/180)
    for i in range(1,len(ys)):
        updt = ys[i-1] + tans[i]
        ys[i] = updt

    return xs,ys

# effective thetas
def thetas(x, y):
    theta = np.repeat(-1.0,len(x))
    theta[0] = 0.0
    for i in range(1,len(x)):
        s = np.sum(theta[:i])
        theta[i] = math.atan((y[i] - y[i-1]) / (x[i] - x[i-1]) ) * 180/math.pi
        theta[i] = theta[i] - s

    if np.any(np.array(theta) < 0):
        raise Exception("Negative angles.")
    return theta

def plot_inlet(angles,flip=True):
    xs,ys = xy(angles)
    fig = plt.figure()
    ax = fig.add_subplot()
    ys = np.max(ys)-ys
    for i in range(len(xs)):
        ax.plot(xs[i:(i+2)],ys[i:(i+2)]);

    ax.hlines(np.max(ys),0,np.max(xs)*1.25)
    ax.hlines(np.min(ys),np.max(xs),np.max(xs)*1.25)
    ax.hlines(np.min(ys),np.max(xs),np.max(xs)*1.25)
    ax.hlines(-0.1*(np.max(ys)-np.min(ys)),0.9*np.max(xs),np.max(xs)*1.25)

    ax.set_aspect('equal', adjustable='box')
    return fig, ax

## HELPER FUNCTIONS

import pandas as pd
from itertools import product
import numpy as np

from scipy.optimize import minimize_scalar

#Prandtl-Meyer
def nu(M, gamma):
    return math.sqrt((gamma + 1) / (gamma - 1)) * math.atan(
        math.sqrt((gamma - 1) / (gamma + 1) * (M ** 2 - 1))) - math.atan(math.sqrt(M ** 2 - 1))

def expansion_mach(M1,theta,gamma):
    A = theta*math.pi/180+nu(M1,gamma)
    loss = lambda M2: (nu(M2,gamma)-A)**2
    res = minimize_scalar(loss, bounds=(1, 10), method='bounded')
    return(res.x)

def expansion_p(p1,M1,M2,gamma):
    top = 1+(gamma-1)/2*M1**2
    bottom = 1+(gamma-1)/2*M2**2
    r = (top/bottom)**(gamma/(gamma-1))
    return p1*r

def compression_beta(theta, mach, gamma):
    n = 0 # 0 = weak shock, 1 = strong shock
    theta = theta * math.pi/180.0;
    mu = math.asin(1/mach);
    c = math.tan(mu)**2;
    a = ((gamma-1)/2 + (gamma+1) * c/2) * math.tan(theta);
    b = ((gamma+1)/2 + (gamma+3) * c/2) * math.tan(theta);
    d = math.sqrt(4*(1-3*a*b)**3/((27*a**2*c+9*a*b-2)**2)-1);
    beta = math.atan((b+9*a*c)/(2*(1-3*a*b))-(d*(27*a**2*c+9*a*b-2))/(6*a*(1-3*a*b))*math.tan(n*math.pi/3+1/3*math.atan(1/d)))*180.0/math.pi
    return beta

def compression_mach(mach1, theta, beta, gamma):
    theta = theta * math.pi/180.0;
    beta = beta * math.pi/180.0;
    mach2 = (1/math.sin(beta - theta)) * math.sqrt((1 + 0.5*(gamma-1)*mach1**2*math.sin(beta)**2)/(gamma*mach1**2*math.sin(beta)**2 - 0.5*(gamma-1)))
    return mach2

def p_to_p_tot(p, mach, gamma):
    p_tot = p / ((1 + 0.5*(gamma-1)*mach**2)**(-1*gamma / (gamma-1)))
    return p_tot

def compression_p(mach1, p1, beta, gamma):
    beta = beta * math.pi/180.0
    p2 = p1*((2*gamma*mach1**2*math.sin(beta)**2 - (gamma-1)) / (gamma+1))
    return p2

## RUN MODEL

def run_model(mach_inf,p_inf,angles,aoa=0,gamma=1.4,verbose=False):

    x,y = xy(angles)

    mach = []
    beta = []
    p = []
    p_tot = []

    # Initialize Region 0
    mach.append(mach_inf)
    beta.append(0)
    p.append(p_inf)
    p_tot.append(p_to_p_tot(p[0], mach[0], gamma))

    theta = thetas(x, y)
    effective_theta = theta#[theta[0], theta[1]+aoa, theta[2], theta[3], theta[4], theta[5]]
    effective_theta[1] = effective_theta[1] + aoa
    if np.sum(effective_theta[:-1])>=90:
        raise Exception("Sum of effective angles is bigger than 90.")


    # Solve for Regions 1 through 5
    for i in range(1, len(theta)):
        beta.append(compression_beta(effective_theta[i], mach[i-1], gamma))
        mach.append(compression_mach(mach[i-1], effective_theta[i], beta[i], gamma))
        p.append(compression_p(mach[i-1], p[i-1], beta[i], gamma))
        p_tot.append(p_to_p_tot(p[i], mach[i], gamma))

    if verbose:
        print(mach)
        print(beta)
        print(p)
        print(p_tot)

    mach_output = mach[len(mach)-1]
    p_tot_output = p_tot[len(p_tot)-1]
    p_tot_inf = p_inf/((1+mach_inf**2*(gamma-1)/2)**(-gamma/(gamma-1)))

    return {'mach':mach_output, 'pr':p_tot_output/p_tot_inf}

"""### **User Input and Array Set up**

*any angles above 68 will give a math domain error. A run_model issue, not something we can fix.*
"""

# User Input for angles and array set up
# Works for all angles
import math

# Input
Angle_number = int(input("Number of Desired Angles:"))

''' line by line explanation
function int() on function input() makes sure that whatever the person
types is a whole number since you can't have half an angle.
'''

# Math
Range = range(0, Angle_number)
X = 36/Angle_number

''' line by line explanation
range() function creates an array of numbers 0 to a specified number
X is a common scaling factor for the angles. Essentially the purpose of
this code is to take the angle input, create a list of that many numbers,
scale the list so that the sum equals 36, sort the list in ascending order.
'''

# Precision only needed up to 27 angles
if Angle_number < 27:

# Set array in ln() function domain
  array = []
  for number in Range:
    number = number + 1
    array.append(X*np.log(number*Angle_number))

''' line by line explanation
if you print(Range) you will see that it starts with 0. Since the ideal
angle graphs (created and tested on the inlet.py code) resemble a flipped
natural log function, we can use this math function to 'fit' the angles to
this shape. The visualization of the steps so far is shown in the link below:
https://witeboard.com/768f7330-3d2d-11ef-b0bc-cb8f1908a6bf
'''

# Remove any error cushion
pillow = abs(sum(array)-36)/(Angle_number)

new_array = []
for i in array:
  i = i-pillow
  new_array.append(i)

''' line by line explanation
The pillow is a measure of how far away our desired values are from our
obtained values. By dividing it by the number of angles we have, we get a
"cushion" value for each number in the array. The hope is that once we subtract,
this cushion from each number in the array we will get the desired sum of 36.
We can do this easily using a for loop and putting the results in a new array,
creatively named new_array. The visualization of these steps is shown here:
https://witeboard.com/e593fbe0-3d30-11ef-a2d1-4f354a32b8a3
'''

# Remove possibility of negative angles
if Angle_number < 27:

  if Angle_number >= 6:
    new_array[0] = -new_array[0]
    new_array[-1] = new_array[-1] - new_array[0] - new_array[0]
    new_array.sort()

  if Angle_number >= 13:
    new_array[0] = -new_array[0]
    new_array[-1] = new_array[-1] - new_array[0] - new_array[0]
    new_array.sort()

  if Angle_number >= 17:
    new_array[0] = -new_array[0]
    new_array[-1] = new_array[-1] - new_array[0] - new_array[0]
    new_array.sort()

  if Angle_number >= 20:
    new_array[0] = -new_array[0]
    new_array[-1] = new_array[-1] - new_array[0] - new_array[0]
    new_array.sort()

else:
  new_array = np.array(Angle_number * [36/Angle_number])

''' line by line explanation
If the pillow value is greater than the average number in the angle, we will
get a negative angle which cannot be passed through the run_model function.
We can experimentally find where this happens by plugging in different angle
amounts. We can see this happen at 6,13,17, and 20. For each of these we take the
first value (the negative one), make it positive, then account for the change in
array sum by subtracting the amount we gained on the last (or highest) number in
the array which is denoted by [-1]. Then we sort the array in ascending order.
'''

test_array = new_array

# Print results / check our work
print(new_array)
print(sum(new_array))
result = run_model(mach_inf=5, p_inf=5532, angles=new_array)
print(result['mach'])
print(result['pr'])

''' line by line explanation
We should get a result that is close to 2.5 for mach; however,
there is error that comes from the issues we have to tip-toe around
such as negative numbers and logrithmic function domains.
'''

# Going forward : making this system for the neural net optimizer

"""### **Optimization and Results Using Neural Network**

**Creation of random training data (no repeat)**
"""

import random

# Polynomial line of best fit for sample scaling
# This was achieved with testing and excel graphs

EndingScalar = (-0.001*(Angle_number**3)) + (0.072*(Angle_number**2)) - (2.1104*Angle_number) + 27.468
SimplifiedScalar = round(EndingScalar, 3)

''' line by line explanation
The Scalar equation was found experimentally to make sure the array of values
in generated strings are within the math domain of the original function. For
example, 3 (Angle_number) angles cannot exceed 30 (EndingScalar) degrees because
total cannot exceed 90 (optimization constraint) degrees. Then we round the value
to 3 decimal places.
'''

# Sample creation
L = np.linspace(0.01, SimplifiedScalar, 40).tolist()
results = []
Sample_Size = 3000
Range = Sample_Size

''' line by line explanation
L is an array of values for the following functions to pick from. Think of it
like a word bank for a word search. Our results matrix is initiated with []. Its
current length is zero because there is nothing in it, remember that for the next
explanation. The sample size is adjustable. The more samples you have, the more
accurate your results will be (and longer the fitting time). 4000 Sample size will
produce a results matrix of 4000 rows.
'''

while len(results) != Range:
  random_line = random.sample(L, Angle_number)
  if 30 < sum(np.array(random_line)) < 40:
    et = [0] + random_line
    et.sort()
    result = run_model(mach_inf=5,p_inf=5532,angles=np.array(et))
    results.append(et + [result['mach']] + [result['pr']])

''' line by line explanation
The while loop reads as follows: While the length of the results matrix is not
equal to the sample size, the random.sample() function is used to pick numbers
from the variable L. The random sample lines are then put through an 'if'
statement. If the sum of the chosen values are between 30 to 40 (aka if the mach
number is between 2.3 to 2.8). Then each line is put through the run_model
function and the results are appended to the results matrix. Once the matrix
fills up to the sample size, the while loop breaks.
'''

randmatrix = np.array(results)
randmatrix = randmatrix.reshape(Range, Angle_number+3)
location = randmatrix[:,1:-2]
mach = randmatrix[:,-2]
pr = randmatrix[:,-1]

''' line by line explanation
This formats the results into a usable matrix where variables can be assigned
via columns and rows. The location makes up the first columns. The last 2 columns
are their respective mach number and pressure.
'''

# Print our results / check our work
print(location)
print(mach)
print(pr)

"""**Creation of Neural Network (no repeat)**"""

import numpy as np
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import BatchNormalization, Dropout
from keras.callbacks import ModelCheckpoint

# Variables defined earlier
X1 = location
y1 = mach
X1 = np.array(X1)
y1 = np.array(y1)

X2 = location
y2 = pr
X2 = np.array(X2)
y2 = np.array(y2)

''' line by line explaination
location is our input becuase of our defining function: J(theta) which will be defined later
The first list of variables will be mapped to our mach neural network
The second list will be for our pressure neural network
'''

# Split data into training and testing sets
X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size=0.2, random_state=42)
X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.2, random_state=42)

''' line by line explaination
the train_test_split is a useful tool to initiate our training and test sets of data
It splits our data into 80% training and 20% testing randomly with a random state of 42
You can put any integer here as a "seed" value, but 42 is a popular choice:
documentation: https://scikit-learn.org/stable/glossary.html#term-random_state and
42: https://en.wikipedia.org/wiki/Phrases_from_The_Hitchhiker%27s_Guide_to_the_Galaxy#The_Answer_to_the_Ultimate_Question_of_Life,_the_Universe,_and_Everything_is_42
'''

# Set a custom activation to guide neural net on the right path
def custom_activation(x):
  return tf.maximum(x, 0.1)

''' line by line explaination
using 'relu' does not work here because although we need the answer to be non-negative (can't have a negative angle)
but we also need the angle to be non-zero (can't divide by zero in the run_model function)
This custom activation speeds up the learning process for the neural net ///
is what i WOULD say if it applied here, saving this just in case we need it later however
'''

# Define the model architecture
model1 = Sequential()
model1.add(Dense(64, activation='relu', input_dim=Angle_number))
model1.add(BatchNormalization())
model1.add(Dense(64, activation='relu'))
model1.add(BatchNormalization())
model1.add(Dense(64, activation='relu'))
model1.add(Dropout(0.25))
model1.add(Dense(1, activation='relu'))

model2 = Sequential()
model2.add(Dense(64, activation='relu', input_dim=Angle_number))
model2.add(BatchNormalization())
model2.add(Dense(64, activation='relu'))
model2.add(BatchNormalization())
model2.add(Dense(64, activation='relu'))
model2.add(Dropout(0.25))
model2.add(Dense(1, activation='relu'))

''' line by line explanation
The first layer is a dense layer with 4 inputs because of our angle amount
The layers inbetween are chose at random almost, just having enough neurons to get a good output
The last layer is 1 neurons because our mach/pressure column is only 1 number
'''

# If we want to save the model, use this callback in the model definition
#checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    #filepath='best_model.h5',
    #monitor='val_loss',
    #mode='min',
    #save_best_only=True,
    #verbose=1)


# Compile the model
model1.compile(optimizer='adam', loss='mse')
model2.compile(optimizer='adam', loss='mse')

''' line by line explanation
Adam is an advanced optimizer that will help our model. It is too advanced for me to
explain or fully comprehend unfortunatley. The loss variable is the mean squared error.
That just means our loss will tell us the square differnce between our predicted and actual values.
'''

# Print model summary / check our work
model1.summary()
model2.summary()

''' line by line explaination
summary() is the model version of print()
use this to see what the model looks like
'''

"""**Train Neural Network (no repeat)**"""

# Teaching our model the random sample data, change epochs based on fit
epochs = 300
history1 = model1.fit(X_train1, y_train1, epochs=epochs, validation_split=0.2)
history2 = model2.fit(X_train2, y_train2, epochs=epochs, validation_split=0.2)

''' line by line explanation
fit function teaches the neural net the sample data created in the previous step
validation_split is the fraction of the data that will be saved and used for validation
for example, if you were to check to see the accuracy of the neural net outputs, it needs
something to compare to check error
'''

"""## **Defining the Objective Function and Optimizing the Original Function**"""

# Import sci tools py // Plan on using NLopt in the future
from scipy.optimize import minimize

def P(theta):
  return run_model(mach_inf=5,p_inf=5532,angles=np.array([0] + theta))['pr']

def M(theta):
  return run_model(mach_inf=5,p_inf=5532,angles=np.array([0] + theta))['mach']

'''line by line explanation
the objective function has 3 variables. We define 2 of those here.
Basically how we assign mach and pressure results but in 1 line of code now
because we've written out enough of those to make this legible now.
Remember to add the zero first for the 'input' code to work (et variable)!
'''

# Objective function to maximize
def J(theta):
    pressure_ratio = P(theta)
    mach_number = M(theta)
    objective = pressure_ratio - ((abs(mach_number - 2.5) / 2.5) / 1.501)
    return -objective

''' line by line explanation
The objective function, J(theta), is defined here. This could be simplified
but it is written out to make the equation easier to read. The resulting
variable, objective, is output from this function using the return statement.
Remember we are trying to MAXIMIZE this function. One way we can do this is to
negate the objective function (the negative sign) and then minimize it. As the
objective varible gets more negative, it gets closer to the minimum; thus,
maximizing it once we take the number output (not the signage!).
1.501 is used as a weight for mach to not be too precise. If you have any
questions of that just ask I'll be able to explain better on the whiteboard.
'''

# Initial guess for theta
#initial_theta = np.array([3, 7, 11, 13])
initial_theta = Angle_number * [36/Angle_number]

# Bounds for theta
#bounds = [(0, 15), (0, 15), (0, 15), (0, 15)]
bounds = Angle_number * [(0.01, (75/Angle_number))]

# Optimization - (swap test_array with initial_theta)
opt_result = minimize(J, test_array, bounds=bounds)

''' line by line explanation
The minimize function requires a special syntax: (def function, guess, bounds)
Our defined function is the objective function (the object we are trying to minimize)
It also requires that we provide it with a 'guess' for what would be good
angles. Since we've played with the 'input' code enough at this point, we have that.
The bounds syntax is optional; however, if you remember the issue we faced before
when we had to make the custom_activation for the Neural Network setup, we can't
have negative angles. We also kill 2 birds with 1 stone by setting a high bound
by preventing the total angles to add up to more than 90 degrees which run_model
would reject and throw an error at.
documentation: https://docs.scipy.org/doc/scipy/tutorial/optimize.html
'''

# Optimized theta
optimized_theta_original_function = opt_result.x
print("Optimized theta:", optimized_theta_original_function)
print("Maximized J(theta):", -opt_result.fun)
print("Corresponding P(theta):", P(optimized_theta_original_function))
print("Corresponding M(theta):", M(optimized_theta_original_function))

''' line by line explanation
our result variable gets lots of useful info after we call the minimize function
you can view them by doing print(result). One of the 2 essential things we need from
result is the angles where the function is optimized. This is denoted by .x in the
documentation.
x: The optimal values of the decision variables theta that minimize the objective function.
The other essential value we need is the optimization value. Because of how our
ojective function is set up, it will be a number between 0 and 1. This is denoted
by .fun in the documentation.
fun: The value of the objective function at the optimal point.
'''

"""**Optimize Neural Network**"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
# Need to install nlopt on first use, % denotes command prompt usage for Google Colab
# %pip install nlopt
import nlopt

def objective(theta, grad):
    P_theta = model2.predict(np.array([theta]))
    M_theta = model1.predict(np.array([theta]))

    ''' line by line explanation
    The objection function syntax for NLopt requires 2 inputs. I used theta and
    grad here. Grad isn't used so technically it could be anything. Theta is the
    total array of the angles. As we pass the angles through the model using the
    .predict() function, it will return the Pressure (P_theta) and Mach (M_theta)
    and assign them to variables.
    '''

    # Convert predictions to scalars
    P_theta = P_theta.flatten()[0]
    M_theta = M_theta.flatten()[0]

    ''' line by line explanation
    The predictions are passed through the .flatten() function. Though these
    functions may not be needed, it ensures that the variables are in the right
    format for the objective function. The [0] means that only 1 prediction is
    returned, and the .flatten() function returns a 1D array.
    '''

    # Define the objective function J(theta)
    J_theta = P_theta - ((abs(M_theta - 2.5) / 2.5) / 1.501)

    return J_theta

    ''' line by line explanation
    The objective function is defined here. Each formatted prediction
    is passed through the function and the result (J_theta) is returned.
    1.501 is used as a weight and to calibrate the objective function.
    This one isn't perfect and I wasn't able to get it closer to 2.5.
    '''


# Example optimization setup using nlopt
def optimize():
    opt = nlopt.opt(nlopt.LN_COBYLA, Angle_number)
    opt.set_lower_bounds(Angle_number * [0.01])
    opt.set_upper_bounds(Angle_number * [20])
    opt.set_max_objective(objective)

    ''' line by line explanation
    The optimize function is defined here. NLopt requires us to tell it
    how we want to optimize the objective function. For the nlopt.opt function:
    LN_COBYLA is an algorithm used for quick converging functions and 4 is the
    number of variables we want to optimize. We can set the range for each angle,
    I used 1 to 15. 1 keeps each angle from being negative and 15 keeps each
    angle from adding up to more than 90 degrees. Since we want to maximize the
    J(theta) function, we use the function: opt.set_max_objective(objective)
    Documentation: https://nlopt.readthedocs.io/en/latest/NLopt_Python_Reference/
    '''

    # Tolerance Functions
    opt.set_xtol_rel(1e-7)
    opt.set_ftol_rel(1e-7)

    ''' line by line explanation
    Without the above functions, the optimize function will not converge in
    reasonable time. This is because it is finding the exact location where these
    functions connect with too much accuracy. We can lower this accuracy (and time
    it takes to converge) by setting the tolerance functions. As tolerance increases
    the accuracy decreases and the time it takes to give an answer is more reasonable.
    '''

    # Initial guess for theta
    initial_theta = optimized_theta_original_function

    # Run the optimization
    theta_opt = opt.optimize(initial_theta)
    max_objective_value = opt.last_optimum_value()

    return theta_opt, max_objective_value

    ''' line by line explanation
    Like the scipy.optimize() functions we used previously, we need to provide
    an initial educated guess for what the solution could be. The optmized angles
    and objective function values are defined to the functions .optimize() and
    .last_optimum_value() respectively. Once again, these can be found on the
    NLopt documentation. Lastly, the optimize function returns the optimal angles
    (theta_opt) and the objective function value (max_objective_value) in that order.
    '''

# Map each variable to the defined optimize function
optimal_theta, max_value = optimize()

''' line by line explanation
Since the defined optimized function returns the angles and J(theta) value in
that order, we need to assign them to variables in that order. In short,
theta_opt is now optimal_theta and max_objective_value is now max_value when
we set them equal to the optimize function.
'''

def P(theta):
  return run_model(mach_inf=5,p_inf=5532,angles=np.array([0] + theta))['pr']

def M(theta):
  return run_model(mach_inf=5,p_inf=5532,angles=np.array([0] + theta))['mach']

''' line by line explanation
These functions just simplified the run_model outputs. This is made because
our accuracy checks below are very long even with this simplification.
'''

# Print results / check our work
#print("Optimal theta:", optimal_theta)
#print("Maximum objective value:", max_value)

# Print results / check our work
#print("Pressure:", P(optimal_theta))
#print("Mach Number:", M(optimal_theta))

print("Angle Number", Angle_number)

optimal_theta.sort()
print("Sorted Pressure", P(optimal_theta))
print("Sorted Mach", M(optimal_theta))
print("Optimized Theta", optimal_theta)

accuracy_mach = 100 - ((np.abs(M(optimized_theta_original_function) - M(optimal_theta)) / M(optimized_theta_original_function)) * 100)
print("Mach Accuracy", accuracy_mach)

accuracy_pressure = 100 - ((np.abs(P(optimized_theta_original_function) - P(optimal_theta)) / P(optimized_theta_original_function)) * 100)
print("Pressure Accuracy", accuracy_pressure)

''' line by line explanation
These show the outputs of optimization and uses regular % error formulas to check
our answers with the scipy optimization package.
'''