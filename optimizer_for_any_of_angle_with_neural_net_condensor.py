# -*- coding: utf-8 -*-
"""Optimizer for any # of angle with Neural Net Condensor

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Tho_2oJvxp20MPpudHr-LwYtR0kXe0CK

### **Given Functions**
"""

## Given Functions

## Given functions provided by W&M
# Needed as pre req for other coding functions used

import matplotlib.pyplot as plt
import csv
import math
import numpy as np

def xy(ethetas):
    thetas = np.cumsum(ethetas)
    xs = np.arange(0,len(ethetas))
    ys = np.repeat(-1.0,len(ethetas))
    ys[0] = 0
    tans = np.tan(thetas*np.pi/180)
    for i in range(1,len(ys)):
        updt = ys[i-1] + tans[i]
        ys[i] = updt

    return xs,ys

# effective thetas
def thetas(x, y):
    theta = np.repeat(-1.0,len(x))
    theta[0] = 0.0
    for i in range(1,len(x)):
        s = np.sum(theta[:i])
        theta[i] = math.atan((y[i] - y[i-1]) / (x[i] - x[i-1]) ) * 180/math.pi
        theta[i] = theta[i] - s

    if np.any(np.array(theta) < 0):
        raise Exception("Negative angles.")
    return theta

def plot_inlet(angles,flip=True):
    xs,ys = xy(angles)
    fig = plt.figure()
    ax = fig.add_subplot()
    ys = np.max(ys)-ys
    for i in range(len(xs)):
        ax.plot(xs[i:(i+2)],ys[i:(i+2)]);

    ax.hlines(np.max(ys),0,np.max(xs)*1.25)
    ax.hlines(np.min(ys),np.max(xs),np.max(xs)*1.25)
    ax.hlines(np.min(ys),np.max(xs),np.max(xs)*1.25)
    ax.hlines(-0.1*(np.max(ys)-np.min(ys)),0.9*np.max(xs),np.max(xs)*1.25)

    ax.set_aspect('equal', adjustable='box')
    return fig, ax

## HELPER FUNCTIONS

import pandas as pd
from itertools import product
import numpy as np

from scipy.optimize import minimize_scalar

#Prandtl-Meyer
def nu(M, gamma):
    return math.sqrt((gamma + 1) / (gamma - 1)) * math.atan(
        math.sqrt((gamma - 1) / (gamma + 1) * (M ** 2 - 1))) - math.atan(math.sqrt(M ** 2 - 1))

def expansion_mach(M1,theta,gamma):
    A = theta*math.pi/180+nu(M1,gamma)
    loss = lambda M2: (nu(M2,gamma)-A)**2
    res = minimize_scalar(loss, bounds=(1, 10), method='bounded')
    return(res.x)

def expansion_p(p1,M1,M2,gamma):
    top = 1+(gamma-1)/2*M1**2
    bottom = 1+(gamma-1)/2*M2**2
    r = (top/bottom)**(gamma/(gamma-1))
    return p1*r

def compression_beta(theta, mach, gamma):
    n = 0 # 0 = weak shock, 1 = strong shock
    theta = theta * math.pi/180.0;
    mu = math.asin(1/mach);
    c = math.tan(mu)**2;
    a = ((gamma-1)/2 + (gamma+1) * c/2) * math.tan(theta);
    b = ((gamma+1)/2 + (gamma+3) * c/2) * math.tan(theta);
    d = math.sqrt(4*(1-3*a*b)**3/((27*a**2*c+9*a*b-2)**2)-1);
    beta = math.atan((b+9*a*c)/(2*(1-3*a*b))-(d*(27*a**2*c+9*a*b-2))/(6*a*(1-3*a*b))*math.tan(n*math.pi/3+1/3*math.atan(1/d)))*180.0/math.pi
    return beta

def compression_mach(mach1, theta, beta, gamma):
    theta = theta * math.pi/180.0;
    beta = beta * math.pi/180.0;
    mach2 = (1/math.sin(beta - theta)) * math.sqrt((1 + 0.5*(gamma-1)*mach1**2*math.sin(beta)**2)/(gamma*mach1**2*math.sin(beta)**2 - 0.5*(gamma-1)))
    return mach2

def p_to_p_tot(p, mach, gamma):
    p_tot = p / ((1 + 0.5*(gamma-1)*mach**2)**(-1*gamma / (gamma-1)))
    return p_tot

def compression_p(mach1, p1, beta, gamma):
    beta = beta * math.pi/180.0
    p2 = p1*((2*gamma*mach1**2*math.sin(beta)**2 - (gamma-1)) / (gamma+1))
    return p2

## RUN MODEL

def run_model(mach_inf,p_inf,angles,aoa=0,gamma=1.4,verbose=False):

    x,y = xy(angles)

    mach = []
    beta = []
    p = []
    p_tot = []

    # Initialize Region 0
    mach.append(mach_inf)
    beta.append(0)
    p.append(p_inf)
    p_tot.append(p_to_p_tot(p[0], mach[0], gamma))

    theta = thetas(x, y)
    effective_theta = theta#[theta[0], theta[1]+aoa, theta[2], theta[3], theta[4], theta[5]]
    effective_theta[1] = effective_theta[1] + aoa
    if np.sum(effective_theta[:-1])>=90:
        raise Exception("Sum of effective angles is bigger than 90.")


    # Solve for Regions 1 through 5
    for i in range(1, len(theta)):
        beta.append(compression_beta(effective_theta[i], mach[i-1], gamma))
        mach.append(compression_mach(mach[i-1], effective_theta[i], beta[i], gamma))
        p.append(compression_p(mach[i-1], p[i-1], beta[i], gamma))
        p_tot.append(p_to_p_tot(p[i], mach[i], gamma))

    if verbose:
        print(mach)
        print(beta)
        print(p)
        print(p_tot)

    mach_output = mach[len(mach)-1]
    p_tot_output = p_tot[len(p_tot)-1]
    p_tot_inf = p_inf/((1+mach_inf**2*(gamma-1)/2)**(-gamma/(gamma-1)))

    return {'mach':mach_output, 'pr':p_tot_output/p_tot_inf}

"""### **User Input and Array Set up**

*any angles above 68 will give a math domain error. A run_model issue, not something we can fix.*
"""

# User Input for angles and array set up
# Works for all angles
import math

# Input
Angle_number = int(input("Number of Desired Angles:"))

''' line by line explanation
function int() on function input() makes sure that whatever the person
types is a whole number since you can't have half an angle.
'''

# Math
Range = range(0, Angle_number)
X = 36/Angle_number

''' line by line explanation
range() function creates an array of numbers 0 to a specified number
X is a common scaling factor for the angles. Essentially the purpose of
this code is to take the angle input, create a list of that many numbers,
scale the list so that the sum equals 36, sort the list in ascending order.
'''

# Precision only needed up to 27 angles
if Angle_number < 27:

# Set array in ln() function domain
  array = []
  for number in Range:
    number = number + 1
    array.append(X*np.log(number*Angle_number))

''' line by line explanation
if you print(Range) you will see that it starts with 0. Since the ideal
angle graphs (created and tested on the inlet.py code) resemble a flipped
natural log function, we can use this math function to 'fit' the angles to
this shape. The visualization of the steps so far is shown in the link below:
https://witeboard.com/768f7330-3d2d-11ef-b0bc-cb8f1908a6bf
'''

# Remove any error cushion
pillow = abs(sum(array)-36)/(Angle_number)

new_array = []
for i in array:
  i = i-pillow
  new_array.append(i)

''' line by line explanation
The pillow is a measure of how far away our desired values are from our
obtained values. By dividing it by the number of angles we have, we get a
"cushion" value for each number in the array. The hope is that once we subtract,
this cushion from each number in the array we will get the desired sum of 36.
We can do this easily using a for loop and putting the results in a new array,
creatively named new_array. The visualization of these steps is shown here:
https://witeboard.com/e593fbe0-3d30-11ef-a2d1-4f354a32b8a3
'''

# Remove possibility of negative angles
if Angle_number < 27:

  if Angle_number >= 6:
    new_array[0] = -new_array[0]
    new_array[-1] = new_array[-1] - new_array[0] - new_array[0]
    new_array.sort()

  if Angle_number >= 13:
    new_array[0] = -new_array[0]
    new_array[-1] = new_array[-1] - new_array[0] - new_array[0]
    new_array.sort()

  if Angle_number >= 17:
    new_array[0] = -new_array[0]
    new_array[-1] = new_array[-1] - new_array[0] - new_array[0]
    new_array.sort()

  if Angle_number >= 20:
    new_array[0] = -new_array[0]
    new_array[-1] = new_array[-1] - new_array[0] - new_array[0]
    new_array.sort()

else:
  new_array = np.array(Angle_number * [36/Angle_number])

''' line by line explanation
If the pillow value is greater than the average number in the angle, we will
get a negative angle which cannot be passed through the run_model function.
We can experimentally find where this happens by plugging in different angle
amounts. We can see this happen at 6,13,17, and 20. For each of these we take the
first value (the negative one), make it positive, then account for the change in
array sum by subtracting the amount we gained on the last (or highest) number in
the array which is denoted by [-1]. Then we sort the array in ascending order.
'''

# Print results / check our work
print(new_array)
print(sum(new_array))
result = run_model(mach_inf=5, p_inf=5532, angles=new_array)
print(result['mach'])
print(result['pr'])

''' line by line explanation
We should get a result that is close to 2.5 for mach; however,
there is error that comes from the issues we have to tip-toe around
such as negative numbers and logrithmic function domains.
'''

# Going forward : making this system for the neural net optimizer

"""### **Optimization and Results Using SciPy**"""

# Import sci tools py // Plan on using NLopt in the future
from scipy.optimize import minimize

def P(theta):
  return run_model(mach_inf=5,p_inf=5532,angles=np.array([0] + theta))['pr']

def M(theta):
  return run_model(mach_inf=5,p_inf=5532,angles=np.array([0] + theta))['mach']

'''line by line explanation
the objective function has 3 variables. We define 2 of those here.
Basically how we assign mach and pressure results but in 1 line of code now
because we've written out enough of those to make this legible now.
Remember to add the zero first for the 'input' code to work (et variable)!
'''

denom = 2*(Angle_number/(Angle_number+1.8))

if Angle_number >= 22:
  denom = round(denom,3)
else:
  denom = round(denom,2)

''' line by line explanation
We set a custom denomenator here because we want the weight to increase
as the number of angles increases. This is an accuracy trend that we
discovered by testing. The values were also picked experimentally.
We use round() to make the weight less precise and the function easier to solve.
We need slightly more precision as angle number increases however, so we
increase sig figs at 22 and above.
* The denom function is a simple fraction where denom approaches 2 as angle
number approaches infinity *
'''

# Objective function to maximize
def J(theta):
    pressure_ratio = P(theta)
    mach_number = M(theta)
    objective = pressure_ratio - ((abs(mach_number - 2.5) / 2.5) / denom)
    return -objective

''' line by line explanation
The objective function, J(theta), is defined here. This could be simplified
but it is written out to make the equation easier to read. The resulting
variable, objective, is output from this function using the return statement.
Remember we are trying to MAXIMIZE this function. One way we can do this is to
negate the objective function (the negative sign) and then minimize it. As the
objective varible gets more negative, it gets closer to the minimum; thus,
maximizing it once we take the number output (not the signage!).
denom with the round() function is used as a weight for mach to not be too precise.
'''

# Initial guess for theta
initial_theta = np.array(new_array)

# Bounds for theta
bounds = Angle_number*[(0.000001,15)]

# Optimization
opt_result = minimize(J, initial_theta, bounds=bounds)

''' line by line explanation
The minimize function requires a special syntax: (def function, guess, bounds)
Our defined function is the objective function (the object we are trying to minimize)
It also requires that we provide it with a 'guess' for what would be good
angles. Since we've played with the 'input' code enough at this point, we have that.
The bounds syntax is optional; however, if you remember the issue we faced before
when we had to make the custom_activation for the Neural Network setup, we can't
have negative angles. We also kill 2 birds with 1 stone by setting a high bound
by preventing the total angles to add up to more than 90 degrees which run_model
would reject and throw an error at.
documentation: https://docs.scipy.org/doc/scipy/tutorial/optimize.html
'''

# Optimized theta
optimized_theta_original_function = opt_result.x
print("Optimized theta:", optimized_theta_original_function)
print("Maximized J(theta):", -opt_result.fun)
print("Corresponding P(theta):", P(optimized_theta_original_function))
print("Corresponding M(theta):", M(optimized_theta_original_function))

''' line by line explanation
our result variable gets lots of useful info after we call the minimize function
you can view them by doing print(result). One of the 2 essential things we need from
result is the angles where the function is optimized. This is denoted by .x in the
documentation.
x: The optimal values of the decision variables theta that minimize the objective function.
The other essential value we need is the optimization value. Because of how our
ojective function is set up, it will be a number between 0 and 1. This is denoted
by .fun in the documentation.
fun: The value of the objective function at the optimal point.
'''

"""### **Optimization and Results Using Neural Network**

**convert input list to 4 shaped array**
"""

# Format new_array to neural network input format
def reshape_list_to_4(lst):
    while len(lst) != 4:
        if len(lst) < 4:
            # Split the largest elements
            index = lst.index(max(lst))
            split_value = lst[index] / 2
            lst.pop(index)
            lst.append(split_value)
            lst.append(split_value)
        elif len(lst) > 4:
            # Combine the smallest elements
            lst.sort()
            combined_value = lst[0] + lst[1]
            lst = lst[2:]  # Remove the first two elements
            #lst.append(combined_value)
            lst.append(combined_value)

    lst_sum = sum(lst)
    if lst_sum != 36:
        correction = 36 - lst_sum
        lst[-1] += correction

    return lst

def reshape_array_to_4(array):
    while len(array) != 4:
        if len(array) < 4:
            # Split the largest elements
            index = np.argmax(array)
            split_value = array[index] / 2
            array = np.delete(array, index)
            array = np.append(array, [split_value, split_value])
        elif len(array) > 4:
            # Combine the smallest elements
            sorted_indices = np.argsort(array)
            combined_value = array[sorted_indices[0]] + array[sorted_indices[1]]
            array = np.delete(array, sorted_indices[:2])
            array = np.append(array, combined_value)

    # Ensure the sum is 36 (due to floating point precision, a slight correction might be needed)
    array_sum = np.sum(array)
    if array_sum != 36:
        correction = 36 - array_sum
        array[-1] += correction

    return array


if type(new_array) == list:
  test_array = reshape_list_to_4(new_array)
elif type(new_array) == np.ndarray:
  test_array = reshape_array_to_4(new_array)

"""**Creation of random training data (no repeat)**"""

import random

# Ideal ranges for mach numbers between 2.3 to 2.7
L = np.linspace(1,15,10).tolist()

''' line by line explanation
np.linspace() has 3 inputs: starting number, ending number, counting metric
these numbers were chosen to get a output mach number that is within 2.3 to 2.7
keep counting metric to 10 if you want a list that counts by 1 (only integers, no decimals)
print the output, L, to see what it looks like
we use the tolist() function to turn the output into a list, this is important for the random.sample() function
in the future step, which requires a list input
'''

# Inititate variables and matrices
variables = []
results = []
randmatrix = []

''' line by line explaination
these functions create a blank set, which can be added on in the future.
we use these when we want to create a set of numbers and assign them to a variable
When we append in the next step, it will fill this blank set with the results as the for loop runs
Add a print(results) statement in the for loop to see what it looks like as the append function fills up the blank set
'''

# Generate 50 random samples with their respective mach number and pressure
# Change Range based on number of sample data points
Range = 500
Range = range(Range)
for _ in Range:
  et = [0] + random.sample(L, 4)
  result = run_model(mach_inf=5,p_inf=5532,angles=np.array(et))
  results.append(et + [result['mach'],result['pr']])

''' line by line explanation
et is our list of angles, we want it to be [0,#,#,#,#] where # is a random number between 4 and 13
random.sample() returns a set number, 4, random numbers from the list: L
result is the variable I use for the mach number and pressure function, this is the function we want to simplify with our model
appending the results aka putting the results into a matrix so we can use it later. The formal for this matrix is [angles (aka et), mach number, pressure]
check the format of each step by doing:
print(et) *If you write this in the for loop, you will see the results for each iteration;however, if you put it outside you will only see the most recent location results*
print(results)
'''

# Reshape the output of results into a matrix we can use
randmatrix = np.array(results)
randmatrix = randmatrix.reshape(len(Range), 7)

''' line by line explanation
np.array turns our output into a numpy object, which means we can run numpy operators on it like reshape.
The reshape function lets us make it into the exact matrix shape we want (in this case 7 columns, 5 locations and 2 variables)
'''

# Varibales defining each column to use later
location = randmatrix[:,1:5]
mach = randmatrix[:,5]
pr = randmatrix[:,6]
mach_pr = randmatrix[:,5:]

''' line by line explanation
print each of these variables [location, mach, pr, mach_pr] to see what their shape looks like.
compare with the results matrix to see how the syntax works for singling out a column(s)
'''

# Print results / check our work
# print(randmatrix)

''' line by line explanation
print the random matrix and compare how it looks to the results matrix to see how our indexing changed it into something nicer to work with
'''

"""**Creation of Neural Network (no repeat)**"""

import numpy as np
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from keras.models import Sequential
from keras.layers import Dense

# Variables defined earlier
X1 = location
y1 = mach
X1 = np.array(X1)
y1 = np.array(y1)

X2 = location
y2 = pr
X2 = np.array(X2)
y2 = np.array(y2)

''' line by line explaination
location is our input becuase of our defining function: J(theta) which will be defined later
The first list of variables will be mapped to our mach neural network
The second list will be for our pressure neural network
'''

# Split data into training and testing sets
X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size=0.2, random_state=42)
X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.2, random_state=42)

''' line by line explaination
the train_test_split is a useful tool to initiate our training and test sets of data
It splits our data into 80% training and 20% testing randomly with a random state of 42
You can put any integer here as a "seed" value, but 42 is a popular choice:
documentation: https://scikit-learn.org/stable/glossary.html#term-random_state and
42: https://en.wikipedia.org/wiki/Phrases_from_The_Hitchhiker%27s_Guide_to_the_Galaxy#The_Answer_to_the_Ultimate_Question_of_Life,_the_Universe,_and_Everything_is_42
'''

# Set a custom activation to guide neural net on the right path
def custom_activation(x):
  return tf.maximum(x, 0.1)

''' line by line explaination
using 'relu' does not work here because although we need the answer to be non-negative (can't have a negative angle)
but we also need the angle to be non-zero (can't divide by zero in the run_model function)
This custom activation speeds up the learning process for the neural net ///
is what i WOULD say if it applied here, saving this just in case we need it later however
'''

# Define the model architecture
model1 = Sequential()
model1.add(Dense(64, activation='relu', input_dim=4))
model1.add(Dense(32, activation='relu'))
model1.add(Dense(16, activation='relu'))
model1.add(Dense(1, activation='relu'))

model2 = Sequential()
model2.add(Dense(64, activation='relu', input_dim=4))
model2.add(Dense(32, activation='relu'))
model2.add(Dense(16, activation='relu'))
model2.add(Dense(1, activation='relu'))

''' line by line explanation
The first layer is a dense layer with 4 inputs because of our angle amount
The layers inbetween are chose at random almost, just having enough neurons to get a good output
The last layer is 1 neurons because our mach/pressure column is only 1 number
'''

# Compile the model
model1.compile(optimizer='adam', loss='mse')
model2.compile(optimizer='adam', loss='mse')

''' line by line explanation
Adam is an advanced optimizer that will help our model. It is too advanced for me to
explain or fully comprehend unfortunatley. The loss variable is the mean squared error.
That just means our loss will tell us the square differnce between our predicted and actual values.
'''

# Print model summary / check our work
#model1.summary()
#model2.summary()

''' line by line explaination
summary() is the model version of print()
use this to see what the model looks like
'''

"""**Train Neural Network (no repeat)**"""

# Teaching our model the random sample data, change epochs based on fit
history1 = model1.fit(X_train1, y_train1, epochs=400, validation_split=0.2)
history2 = model2.fit(X_train2, y_train2, epochs=400, validation_split=0.2)

''' line by line explanation
fit function teaches the neural net the sample data created in the previous step
validation_split is the fraction of the data that will be saved and used for validation
for example, if you were to check to see the accuracy of the neural net outputs, it needs
something to compare to check error
'''

"""**Optimize Neural Network**"""

import numpy as np
# Need to install nlopt on first use, % denotes command prompt usage for Google Colab
#%pip install nlopt
import nlopt

def objective(theta, grad):
    P_theta = model2.predict(np.array([theta]))
    M_theta = model1.predict(np.array([theta]))

    ''' line by line explanation
    The objection function syntax for NLopt requires 2 inputs. I used theta and
    grad here. Grad isn't used so technically it could be anything. Theta is the
    total array of the angles. As we pass the angles through the model using the
    .predict() function, it will return the Pressure (P_theta) and Mach (M_theta)
    and assign them to variables.
    '''

    # Convert predictions to scalars
    P_theta = P_theta.flatten()[0]
    M_theta = M_theta.flatten()[0]

    ''' line by line explanation
    The predictions are passed through the .flatten() function. Though these
    functions may not be needed, it ensures that the variables are in the right
    format for the objective function. The [0] means that only 1 prediction is
    returned, and the .flatten() function returns a 1D array.
    '''

    # Define the objective function J(theta)
    J_theta = P_theta - ((abs(M_theta - 2.5) / 2.5) / 1.501)

    return J_theta

    ''' line by line explanation
    The objective function is defined here. Each formatted prediction
    is passed through the function and the result (J_theta) is returned.
    1.501 is used as a weight and to calibrate the objective function.
    This one isn't perfect and I wasn't able to get it closer to 2.5.
    '''


# Example optimization setup using nlopt
def optimize():
    opt = nlopt.opt(nlopt.LN_COBYLA, 4)
    opt.set_lower_bounds([1.0, 1.0, 1.0, 1.0])
    opt.set_upper_bounds([15.0, 15.0, 15.0, 15.0])
    opt.set_max_objective(objective)

    ''' line by line explanation
    The optimize function is defined here. NLopt requires us to tell it
    how we want to optimize the objective function. For the nlopt.opt function:
    LN_COBYLA is an algorithm used for quick converging functions and 4 is the
    number of variables we want to optimize. We can set the range for each angle,
    I used 1 to 15. 1 keeps each angle from being negative and 15 keeps each
    angle from adding up to more than 90 degrees. Since we want to maximize the
    J(theta) function, we use the function: opt.set_max_objective(objective)
    Documentation: https://nlopt.readthedocs.io/en/latest/NLopt_Python_Reference/
    '''

    # Tolerance Functions
    opt.set_xtol_rel(1e-7)
    opt.set_ftol_rel(1e-7)

    ''' line by line explanation
    Without the above functions, the optimize function will not converge in
    reasonable time. This is because it is finding the exact location where these
    functions connect with too much accuracy. We can lower this accuracy (and time
    it takes to converge) by setting the tolerance functions. As tolerance increases
    the accuracy decreases and the time it takes to give an answer is more reasonable.
    '''

    # Initial guess for theta
    initial_theta = test_array

    # Run the optimization
    theta_opt = opt.optimize(initial_theta)
    max_objective_value = opt.last_optimum_value()

    return theta_opt, max_objective_value

    ''' line by line explanation
    Like the scipy.optimize() functions we used previously, we need to provide
    an initial educated guess for what the solution could be. The optmized angles
    and objective function values are defined to the functions .optimize() and
    .last_optimum_value() respectively. Once again, these can be found on the
    NLopt documentation. Lastly, the optimize function returns the optimal angles
    (theta_opt) and the objective function value (max_objective_value) in that order.
    '''

# Map each variable to the defined optimize function
optimal_theta, max_value = optimize()

''' line by line explanation
Since the defined optimized function returns the angles and J(theta) value in
that order, we need to assign them to variables in that order. In short,
theta_opt is now optimal_theta and max_objective_value is now max_value when
we set them equal to the optimize function.
'''

# Print results / check our work
print("Optimal theta:", optimal_theta)
print("Maximum objective value:", max_value)

# Print results / check our work
print("Pressure:", P(optimal_theta))
print("Mach Number:", M(optimal_theta))

''' line by line explanation
Print out the results of our optimization (optimal_theta and J(theta) / max_value)
Our issue here is that we are unable to take the results here to find the mach
number and the pressure. This is because our neural networks stricly take 1 input
and give us 4 outputs. Once it gives us the angles, we cannot simply just put them
into the new function and get 1 output (our mach and pr). So, to get any kind of
idea of what these values are, we pass them through the original run_model function
by using the previously defined P(theta) and M(theta) functions. This might not
be ideal for what we are trying to do, but it is good to visualize out accuracy
of the neural network optimization.
'''

"""**Reformat ideal angles to desired angle amount**"""

def reshape_optimal_to_angle_number(beta):
    while len(beta) != int(Angle_number):
        if len(beta) < int(Angle_number):
            # Split the largest elements
            index = np.argmax(beta)
            split_value = beta[index] / 2
            beta = np.delete(beta, index)
            beta = np.append(beta, [split_value, split_value])
        elif len(beta) > int(Angle_number):
            # Combine the smallest elements
            sorted_indices = np.argsort(beta)
            combined_value = beta[sorted_indices[0]] + beta[sorted_indices[1]]
            beta = np.delete(beta, sorted_indices[:2])
            beta = np.append(beta, combined_value)

    # Ensure the sum is 36
    array_sum = np.sum(beta)
    if array_sum != 36:
        correction = 36 - array_sum
        beta[-1] += correction

    return beta

reshaped_optimal_theta = reshape_optimal_to_angle_number(optimal_theta)
reshaped_optimal_theta.sort()
print("Optimal Angles for", Angle_number, "Angles", reshaped_optimal_theta.tolist())
print("Estimated Mach Number", M(reshaped_optimal_theta))
print("Estimated Pressure", P(reshaped_optimal_theta))

